{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from waste_management.config import PROCESSED_DATA_DIR\n",
    "from loguru import logger\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-11 21:51:01.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mLoading the dataset\u001b[0m\n",
      "\u001b[32m2024-07-11 21:51:01.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mdataframe head -                                             filepath      label\n",
      "0  /mnt/e/Programowanie/waste_management/data/raw...  cardboard\n",
      "1  /mnt/e/Programowanie/waste_management/data/raw...  cardboard\n",
      "2  /mnt/e/Programowanie/waste_management/data/raw...  cardboard\n",
      "3  /mnt/e/Programowanie/waste_management/data/raw...  cardboard\n",
      "4  /mnt/e/Programowanie/waste_management/data/raw...  cardboard\u001b[0m\n",
      "\u001b[32m2024-07-11 21:51:01.945\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[32m\u001b[1mDataset loaded successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Log a message indicating that the dataset is being loaded\n",
    "logger.info('Loading the dataset')\n",
    "\n",
    "# Read the dataset from a CSV file\n",
    "df = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, 'dataset.csv'))\n",
    "\n",
    "# Log the first few rows of the dataframe\n",
    "logger.info('dataframe head - {}'.format(df.head()))\n",
    "\n",
    "# Log a success message indicating that the dataset was loaded successfully\n",
    "logger.success('Dataset loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-11 21:54:09.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mSplitting the dataset into train and validation\u001b[0m\n",
      "\u001b[32m2024-07-11 21:54:09.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mNumber of images in the training set: 2021\u001b[0m\n",
      "\u001b[32m2024-07-11 21:54:09.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mNumber of images in the validation set: 506\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train and validation without stratification\n",
    "logger.info('Splitting the dataset into train and validation')\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the number of images in each dataset\n",
    "logger.info('Number of images in the training set: {}'.format(len(train_df)))\n",
    "logger.info('Number of images in the validation set: {}'.format(len(val_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-11 21:56:56.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mClass Distribution for df:\u001b[0m\n",
      "\u001b[32m2024-07-11 21:56:56.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mlabel\n",
      "paper        23.51\n",
      "glass        19.83\n",
      "plastic      19.07\n",
      "metal        16.22\n",
      "cardboard    15.95\n",
      "trash         5.42\n",
      "Name: proportion, dtype: float64\u001b[0m\n",
      "\u001b[32m2024-07-11 21:56:56.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mClass Distribution for train_df:\u001b[0m\n",
      "\u001b[32m2024-07-11 21:56:56.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mlabel\n",
      "paper        23.55\n",
      "glass        19.50\n",
      "plastic      18.95\n",
      "cardboard    16.43\n",
      "metal        15.98\n",
      "trash         5.59\n",
      "Name: proportion, dtype: float64\u001b[0m\n",
      "\u001b[32m2024-07-11 21:56:56.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mClass Distribution for val_df:\u001b[0m\n",
      "\u001b[32m2024-07-11 21:56:56.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mlabel\n",
      "paper        23.32\n",
      "glass        21.15\n",
      "plastic      19.57\n",
      "metal        17.19\n",
      "cardboard    14.03\n",
      "trash         4.74\n",
      "Name: proportion, dtype: float64\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Calculate the class distribution for the entire dataframe\n",
    "df_class_distribution = df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Calculate the class distribution for the training dataset\n",
    "train_class_distribution = train_df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Calculate the class distribution for the validation dataset\n",
    "val_class_distribution = val_df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Log the class distribution for the entire dataframe\n",
    "logger.info(\"Class Distribution for df:\")\n",
    "logger.info(df_class_distribution.round(2))\n",
    "\n",
    "# Log the class distribution for the training dataset\n",
    "logger.info(\"Class Distribution for train_df:\")\n",
    "logger.info(train_class_distribution.round(2))\n",
    "\n",
    "# Log the class distribution for the validation dataset\n",
    "logger.info(\"Class Distribution for val_df:\")\n",
    "logger.info(val_class_distribution.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-11 22:01:15.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mSplitting the dataset into train and validation\u001b[0m\n",
      "\u001b[32m2024-07-11 22:01:15.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mNumber of images in the training set: 2021\u001b[0m\n",
      "\u001b[32m2024-07-11 22:01:15.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mNumber of images in the validation set: 506\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train and validation with stratification\n",
    "logger.info('Splitting the dataset into train and validation')\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "# Print the number of images in each dataset\n",
    "logger.info('Number of images in the training set: {}'.format(len(train_df)))\n",
    "logger.info('Number of images in the validation set: {}'.format(len(val_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-11 22:01:17.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mClass Distribution for df:\u001b[0m\n",
      "\u001b[32m2024-07-11 22:01:17.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mlabel\n",
      "paper        23.51\n",
      "glass        19.83\n",
      "plastic      19.07\n",
      "metal        16.22\n",
      "cardboard    15.95\n",
      "trash         5.42\n",
      "Name: proportion, dtype: float64\u001b[0m\n",
      "\u001b[32m2024-07-11 22:01:17.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mClass Distribution for train_df:\u001b[0m\n",
      "\u001b[32m2024-07-11 22:01:17.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mlabel\n",
      "paper        23.50\n",
      "glass        19.84\n",
      "plastic      19.05\n",
      "metal        16.23\n",
      "cardboard    15.93\n",
      "trash         5.44\n",
      "Name: proportion, dtype: float64\u001b[0m\n",
      "\u001b[32m2024-07-11 22:01:17.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mClass Distribution for val_df:\u001b[0m\n",
      "\u001b[32m2024-07-11 22:01:17.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mlabel\n",
      "paper        23.52\n",
      "glass        19.76\n",
      "plastic      19.17\n",
      "metal        16.21\n",
      "cardboard    16.01\n",
      "trash         5.34\n",
      "Name: proportion, dtype: float64\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Calculate the class distribution for the entire dataframe\n",
    "df_class_distribution = df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Calculate the class distribution for the training dataset\n",
    "train_class_distribution = train_df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Calculate the class distribution for the validation dataset\n",
    "val_class_distribution = val_df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Log the class distribution for the entire dataframe\n",
    "logger.info(\"Class Distribution for df:\")\n",
    "logger.info(df_class_distribution.round(2))\n",
    "\n",
    "# Log the class distribution for the training dataset\n",
    "logger.info(\"Class Distribution for train_df:\")\n",
    "logger.info(train_class_distribution.round(2))\n",
    "\n",
    "# Log the class distribution for the validation dataset\n",
    "logger.info(\"Class Distribution for val_df:\")\n",
    "logger.info(val_class_distribution.round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waste_management_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
